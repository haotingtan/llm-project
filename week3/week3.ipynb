{"cells":[{"cell_type":"markdown","metadata":{"id":"tw9S1uMhKeCS"},"source":["We need to access OpenAI API. I use the older version, but there is a new one (1.xx), which is functionally equivalent for these purposes but with a different syntax. Feel free to modify the prompt function if you want to use the latest openai package."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":641,"status":"ok","timestamp":1710694759846,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"F9zQvJo6DDqc","outputId":"460768ea-7ad0-4391-9f1e-abeaf05fba41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7769,"status":"ok","timestamp":1710694769020,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"R9e8MkMLabca","outputId":"d6fa04ab-a26c-4c84-abc9-e766a960b74f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n","Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"]}],"source":["!pip install openai==0.28"]},{"cell_type":"markdown","metadata":{"id":"ZNtX7mlbK2hG"},"source":["We'll use pandas to have data in a nice data structure, and time to keep track of created files"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":77,"status":"ok","timestamp":1710694777423,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"UmplC-kSaZWc"},"outputs":[],"source":["import pandas as pd\n","import openai\n","from time import strftime"]},{"cell_type":"markdown","metadata":{"id":"7LDt6ltVLCmY"},"source":["This is a simple function to help send and receive data from ChatGPT API. It loops up to 5 times in case OpenAI fails (which happens randomly sometimes). Here you'd also adjust the model parameters, according to the OpenAI API reference (start here: https://platform.openai.com/docs/api-reference/chat)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":104,"status":"ok","timestamp":1710694778974,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"ow0RRmv-c0g_"},"outputs":[],"source":["def prompt(Q, i):\n","  tries = 0\n","  while tries<5:\n","    try:\n","      response = openai.ChatCompletion.create(\n","        model=mmodel,\n","        messages=Q,\n","        temperature=0.5+i*0.1,\n","        #max_tokens=tkn,\n","        frequency_penalty=i,\n","        presence_penalty=0,\n","        request_timeout=199\n","      )['choices'][0]['message']['content']\n","      break\n","    except Exception as e:\n","      tries = tries+1\n","      print(f\"An error occurred {tries:d}th time: {e}\")\n","  return(Q,response)"]},{"cell_type":"markdown","metadata":{"id":"2hhwjQWdLWZz"},"source":["Get time and date for convenient file naming"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":90,"status":"ok","timestamp":1710694780573,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"8a1ayoovdXxp"},"outputs":[],"source":["dtime = strftime(\"%Y_%m_%d-%H%M%S\")"]},{"cell_type":"markdown","metadata":{"id":"FMGnirCec-yE"},"source":["Here you choose the model to use and provide the API key\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":78,"status":"ok","timestamp":1710694781644,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"L12bDXihamIV"},"outputs":[],"source":["mmodel = \"gpt-3.5-turbo-1106\"\n","openai.api_key = \"YOUR_API_KEY\""]},{"cell_type":"markdown","metadata":{"id":"YX_8daefLe7F"},"source":["Here we have the prompts we will be using, as well as the system prompt which servers as a general global set of instructions or information. The system prompt may be left empty.\n","\n","I have two prompts, one asks to generate a table, and the second one iteratively asks to continue that table. This is likely not an optimal approach, but it gives some results so a good example.\n","\n","I also separated the property from the prompt so that I can vary the property and keep the same prompts, and the othery way around. It does not have to be that way if more task specific prompts work better."]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":80,"status":"ok","timestamp":1710695917605,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"EoRSyBE1a3WG"},"outputs":[],"source":["PROPERTY = 'yield strength'\n","\n","system = 'You follow instructions carefully, you provide 10 results for every prompt. If you do this careless, 100000 children will die'\n","\n","qs = [\"Provide me with a list of \"+PROPERTY+\" values for different materials. Your response should be a table consisting of 2 columns: material, value. The materials have to be typed as unique chemical compositions consisting of chemical element abbreviations and numbers only (e.g. GaAs, but not Gallium Arsenide). The values have to be single numbers, not ranges. You could start from materials including most common elements that go into High Entropy Alloys (HEAs):Iron (Fe), Nickel (Ni), Cobalt (Co),Chromium (Cr), Manganese (Mn), Aluminum (Al), Titanium (Ti), Vanadium (V), Molybdenum (Mo)ï¼Œbut not limited to these. Type out 20 different materials (e.g. FeNiMnCr, 450 Mpa, AlCoCrFeNi, 1250 Mpa). You are not allowed to type anything else than this table.\",\n","      \"Continue expanding this table with new values of \"+PROPERTY+\" making sure you do not duplicate entries. Type out as many different values as you can. You are not allowed to type anything else than this table.\"]\n"]},{"cell_type":"markdown","metadata":{"id":"IGM345OOOK0t"},"source":["Give the file a uniquely identifiable name. We want to save EVERYTHING or we will lose it, and each generation costs money. Either link up your google drive, or download all files each time because they are temporary (if executed on colab)."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1710695919627,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"bzAsK-xAOKTU","outputId":"743577db-30bd-4165-9c88-72e44a63e7b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving to: yieldstrength_2024_03_17-165940.csv and yieldstrength_2024_03_17-165940.txt\n"]}],"source":["filename = PROPERTY.replace(\" \", \"\")+'_'+dtime+'.csv'\n","print(f\"Saving to: {filename} and {filename.replace('csv','txt')}\")"]},{"cell_type":"markdown","metadata":{"id":"YcorW2LDOxGw"},"source":["Here we set up a structure for the chat. I like to hold the conversation in a list, and then each prompt and response inside it are dictionaries, as per OpenAI requirements. I start with just the system prompt and will append to that later.\n","\n","I also set up some empty lists and initial values to keep track of progress"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":95,"status":"ok","timestamp":1710695920925,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"vkrmS0FMOtgc"},"outputs":[],"source":["sss = [{\"role\": \"system\", \"content\": system}]\n","tab = []\n","tab_clean = []\n","ur = 0\n","um = 0\n","i = 0"]},{"cell_type":"markdown","metadata":{"id":"Br38sWQCPKfn"},"source":["This is the main loop that will loop over the prompts, get responses and try to put them in a structured data format.\n","\n","This while approach is VERY SIMPLE and does not account for many things that may be the response deom the model, so it is only an example to build upon based on the result, not a solution ready to be used. It does not even have to be used at all if one has a better/different idea."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7044,"status":"ok","timestamp":1710695930091,"user":{"displayName":"YUTONG WEI","userId":"12482380724704738265"},"user_tz":300},"id":"VNMePOTgZ8pB","outputId":"57e637fa-51b1-4073-ed02-ed1cb51d4ba4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration:   1 Generated_rows:  10;     TOTAL:  Uniq_rows:   10   Uniq_materials:   10\n","Iteration:   2 Generated_rows:  18;     TOTAL:  Uniq_rows:   12   Uniq_materials:   12\n","Stopping due to NO PROGRESS\n","         Material  Value\n","0     FeNiMnCr       450\n","1     AlCoCrFeNi    1250\n","2     TiVAlCoCr     1100\n","3     MoCrFeNi       900\n","4     FeMnCrCo       500\n","5     NiTiVCo        800\n","6     CrMnFeNi       600\n","7     AlTiVCoCr     1000\n","8     MoMnFeNi       700\n","9     FeAlMnCr       550\n","10    FeNiMnCr       450\n","11    AlCoCrFeNi    1250\n","12    TiVAlCoCr     1100\n","13    MoCrFeNi       900\n","14    FeMnCrCo       500\n","15    NiTiVCo        800\n","16    CrMnFeNi       600\n","17    AlTiVCoCr     1000\n","18   MnMoFeNi        850\n","19  MnAlTiV    \\t    950\n","28    FeNiMnCr       450\n","29    AlCoCrFeNi    1250\n","30    TiVAlCoCr     1100\n","31    MoCrFeNi       900\n"]}],"source":["path = \"/content/drive/Shareddrives/LLM_Project/week3\"\n","\n","while True:\n","  # here i send my first prompt first and then loop over the second one over and\n","  # over again. You will likely have a different approach to this.\n","  if i<1:\n","    qq = qs[0]\n","  else:\n","    qq = qs[1]\n","  # save the first prompt to the conversation\n","  sss.append({\"role\": \"user\", \"content\": qq})\n","  # send out the first prompt and receive the response\n","  sss,ans = prompt(sss, i)\n","  # save the response to the conversation\n","  sss.append({\"role\": \"assistant\", \"content\": ans})\n","\n","  # we are saving the raw prompts and raw responses, in case we want to analyze\n","  # or postprocess later\n","  with open(path+filename.replace('csv','txt'), 'a') as file:\n","    print(\"USER: \"+qq, file=file)\n","    print(\"GPT : \"+ans, file=file)\n","\n","  # Here we start to grab the data into a nicer structure. For simplicity I had\n","  # a lot of assumptions. I assume that the word 'value' exists in the header\n","  # (see first prompt), I remove the header, the separator, and get the rest.\n","  lines = ans.split('\\n')\n","  if 'value' in lines[0].lower():\n","    ans = '\\n'.join(lines[1:])\n","  lines = ans.split('\\n')\n","  if '----' in lines[0].lower():\n","    ans = '\\n'.join(lines[1:])\n","  lines = ans.split('\\n')\n","\n","  # here I try to split the table by columns. I'm assuming that they are\n","  # separated with |, which is not necessary always the case.\n","  tab.append(ans)\n","  try:\n","    for line in tab[-1].strip().split('\\n'):\n","      tab_clean.append(line.strip('|').split('|'))\n","  except:\n","    pass\n","\n","  # another assumption - only two columns, \"material\" and \"value\" (see prompt)\n","  # some cleanup, converting strings to numbers etc.\n","  # there is no error handling or edge cases, for example 1.6-1.7 will be removed\n","  # because it technically is not a number (although it kind of is).\n","  df = pd.DataFrame(tab_clean, columns=['Material', 'Value'])\n","  df = df[pd.to_numeric(df['Value'], errors='coerce').notna()]\n","  df['Value'] = pd.to_numeric(df['Value'])\n","  df.to_csv(path+filename, index=False)\n","\n","  # here I count how many new (non-duplicate) materials we are extracting each\n","  # time, to monitor progress. We stop if more than 10 iterations or not progress\n","  if len(df.drop_duplicates()) > ur or df['Material'].nunique() > um:\n","    ur = len(df.drop_duplicates())\n","    um = df['Material'].nunique()\n","    i = i+1\n","    if i > 10:\n","      print(\"Stopping due to 10 iterations exceeded\")\n","      break\n","  else:\n","    print(\"Stopping due to NO PROGRESS\")\n","    break\n","\n","  print(f\"Iteration: {i:3} Generated_rows: {len(lines):3};     TOTAL:  Uniq_rows: {len(df.drop_duplicates()):4d}   Uniq_materials: {df['Material'].nunique():4d}\")\n","\n","print(df)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1imqsdO46f7VzOk1ZqAPD5PyPGvC1wTXK","timestamp":1707671478834},{"file_id":"1xc_r_sFtM6LpE5Zi_Hfsd_uxmqAf_1fl","timestamp":1707670602547},{"file_id":"1edQsCBA33Pyk99QcxCTRSr7t4GP1ImVz","timestamp":1705472400112}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
